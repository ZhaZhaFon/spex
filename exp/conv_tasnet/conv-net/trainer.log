2022-09-01 20:33:11 [/home/zhao/code/spex+/libs/trainer.py:174 - INFO ] Create optimizer adam: {'lr': 0.001, 'weight_decay': 1e-05}
2022-09-01 20:33:11 [/home/zhao/code/spex+/libs/trainer.py:141 - INFO ] Model summary:
ConvTasNet(
  (encoder_1d_short): Conv1D(1, 256, kernel_size=(20,), stride=(10,))
  (encoder_1d_middle): Conv1D(1, 256, kernel_size=(80,), stride=(10,))
  (encoder_1d_long): Conv1D(1, 256, kernel_size=(160,), stride=(10,))
  (ln): ChannelWiseLayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (proj): Conv1D(768, 256, kernel_size=(1,), stride=(1,))
  (conv_block_1): Conv1DBlock_v2(
    (conv1x1): Conv1D(512, 512, kernel_size=(1,), stride=(1,))
    (prelu1): PReLU(num_parameters=1)
    (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
    (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512)
    (prelu2): PReLU(num_parameters=1)
    (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
    (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
  )
  (conv_block_1_other): Sequential(
    (0): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (1): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (2): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (3): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (4): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (5): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (6): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
  )
  (conv_block_2): Conv1DBlock_v2(
    (conv1x1): Conv1D(512, 512, kernel_size=(1,), stride=(1,))
    (prelu1): PReLU(num_parameters=1)
    (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
    (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512)
    (prelu2): PReLU(num_parameters=1)
    (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
    (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
  )
  (conv_block_2_other): Sequential(
    (0): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (1): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (2): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (3): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (4): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (5): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (6): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
  )
  (conv_block_3): Conv1DBlock_v2(
    (conv1x1): Conv1D(512, 512, kernel_size=(1,), stride=(1,))
    (prelu1): PReLU(num_parameters=1)
    (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
    (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512)
    (prelu2): PReLU(num_parameters=1)
    (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
    (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
  )
  (conv_block_3_other): Sequential(
    (0): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (1): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (2): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (3): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (4): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (5): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (6): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
  )
  (conv_block_4): Conv1DBlock_v2(
    (conv1x1): Conv1D(512, 512, kernel_size=(1,), stride=(1,))
    (prelu1): PReLU(num_parameters=1)
    (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
    (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512)
    (prelu2): PReLU(num_parameters=1)
    (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
    (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
  )
  (conv_block_4_other): Sequential(
    (0): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (1): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (2): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (3): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (4): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (5): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
    (6): Conv1DBlock(
      (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
      (prelu1): PReLU(num_parameters=1)
      (lnorm1): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=512)
      (prelu2): PReLU(num_parameters=1)
      (lnorm2): GlobalChannelLayerNorm(512, eps=1e-05, elementwise_affine=True)
      (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
    )
  )
  (mask1): Conv1D(256, 256, kernel_size=(1,), stride=(1,))
  (mask2): Conv1D(256, 256, kernel_size=(1,), stride=(1,))
  (mask3): Conv1D(256, 256, kernel_size=(1,), stride=(1,))
  (decoder_1d_1): ConvTrans1D(256, 1, kernel_size=(20,), stride=(10,))
  (decoder_1d_2): ConvTrans1D(256, 1, kernel_size=(80,), stride=(10,))
  (decoder_1d_3): ConvTrans1D(256, 1, kernel_size=(160,), stride=(10,))
  (aux_enc3): Sequential(
    (0): ChannelWiseLayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (1): Conv1D(768, 256, kernel_size=(1,), stride=(1,))
    (2): ResBlock(
      (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
      (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
      (batch_norm1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (prelu1): PReLU(num_parameters=1)
      (prelu2): PReLU(num_parameters=1)
      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
    )
    (3): ResBlock(
      (conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
      (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
      (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (batch_norm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (prelu1): PReLU(num_parameters=1)
      (prelu2): PReLU(num_parameters=1)
      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
      (conv_downsample): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
    )
    (4): ResBlock(
      (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
      (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
      (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (batch_norm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (prelu1): PReLU(num_parameters=1)
      (prelu2): PReLU(num_parameters=1)
      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
    )
    (5): Conv1D(512, 256, kernel_size=(1,), stride=(1,))
  )
  (pred_linear): Linear(in_features=256, out_features=101, bias=True)
)
2022-09-01 20:33:11 [/home/zhao/code/spex+/libs/trainer.py:144 - INFO ] Loading model to GPUs:(0,), #param: 11.14M
2022-09-01 20:33:11 [/home/zhao/code/spex+/libs/trainer.py:215 - INFO ] Set eval mode...
2022-09-01 20:33:45 [/home/zhao/code/spex+/libs/trainer.py:68 - INFO ] Processed 200 batches(loss = +44.28)...
2022-09-01 20:34:17 [/home/zhao/code/spex+/libs/trainer.py:68 - INFO ] Processed 400 batches(loss = +44.73)...
2022-09-01 20:34:50 [/home/zhao/code/spex+/libs/trainer.py:68 - INFO ] Processed 600 batches(loss = +44.01)...
2022-09-01 20:35:23 [/home/zhao/code/spex+/libs/trainer.py:68 - INFO ] Processed 800 batches(loss = +44.40)...
2022-09-01 20:35:56 [/home/zhao/code/spex+/libs/trainer.py:68 - INFO ] Processed 1000 batches(loss = +44.67)...
2022-09-01 20:36:29 [/home/zhao/code/spex+/libs/trainer.py:68 - INFO ] Processed 1200 batches(loss = +44.84)...
2022-09-01 20:37:01 [/home/zhao/code/spex+/libs/trainer.py:68 - INFO ] Processed 1400 batches(loss = +44.58)...
2022-09-01 20:37:18 [/home/zhao/code/spex+/libs/trainer.py:75 - INFO ] Loss on 1500 batches: 41.11,47.44,41.35,46.33,47.61,45.89,41.23,39.99,41.31,55.92,57.90,43.72,46.64,44.01,44.02,48.49,43.79,41.07,40.83,46.96,43.77,46.32,38.58,44.96,45.02,41.06,42.66,40.65,44.81,47.05,44.73,41.80,44.70,49.79,44.89,43.06,42.52,39.70,41.36,51.41,45.92,40.23,43.13,49.68,41.38,41.16,44.65,43.18,52.43,42.30,45.70,45.38,40.81,44.07,44.73,48.17,45.39,38.89,46.01,46.30,41.57,47.68,40.69,41.92,46.45,47.69,44.76,39.30,48.79,40.64,44.39,40.04,37.46,42.16,42.33,46.04,43.96,42.73,40.66,53.60,41.89,46.29,35.70,50.73,42.31,40.74,47.78,44.69,49.17,42.91,41.07,48.71,49.29,44.45,39.50,49.68,38.31,46.33,40.53,50.57,40.61,43.47,49.24,45.39,41.44,49.62,45.47,42.46,40.17,51.02,39.09,46.36,41.01,43.35,44.06,40.56,39.79,50.32,41.51,43.02,47.27,43.86,43.39,40.13,39.45,41.73,43.87,58.99,45.47,43.55,45.21,41.25,41.32,38.74,43.11,44.72,47.50,43.53,46.53,42.21,42.74,47.82,46.32,38.84,39.37,43.85,50.75,45.45,41.54,41.37,41.34,42.80,41.89,47.31,39.00,39.15,45.26,43.40,52.16,38.56,48.06,46.43,40.40,40.03,41.72,48.49,44.88,41.66,52.20,51.47,40.90,46.67,58.45,42.01,47.94,37.50,40.96,38.51,45.51,38.54,44.91,55.26,48.81,52.60,49.85,43.41,44.48,46.90,46.76,47.73,41.61,36.13,42.49,39.54,52.00,40.94,39.00,38.46,37.56,48.08,43.32,45.62,47.69,39.62,47.54,45.15,46.85,43.22,41.92,42.89,42.69,50.15,40.68,45.06,43.65,45.74,48.83,47.13,49.42,45.53,53.79,47.38,31.68,40.62,40.78,40.37,40.48,43.11,46.60,50.35,41.92,51.64,46.20,51.49,42.18,48.18,41.34,44.89,48.06,49.83,53.02,41.79,46.37,40.36,43.99,53.25,43.75,44.61,42.44,42.74,49.22,52.55,47.26,43.67,39.26,38.96,48.54,50.95,45.86,41.74,47.84,41.77,45.13,50.46,56.87,41.10,44.32,42.76,46.64,46.28,45.17,37.64,38.86,46.38,41.70,41.46,40.82,46.63,41.91,41.98,47.17,44.60,43.34,46.19,45.84,37.19,47.38,53.52,51.24,53.32,44.81,48.85,43.39,39.62,49.01,47.47,46.20,48.06,44.45,42.27,41.14,39.88,48.36,43.16,41.70,42.46,42.52,41.35,49.71,52.04,45.62,42.24,43.03,41.42,45.00,42.00,49.22,38.38,47.06,44.14,41.57,40.09,38.17,45.50,39.52,37.98,42.30,43.63,45.82,44.90,48.29,46.46,48.68,42.95,45.82,45.85,44.72,43.42,44.88,44.32,49.70,42.05,48.28,40.76,44.27,45.09,44.53,45.76,44.93,39.47,37.09,41.81,48.64,47.20,54.25,42.41,41.85,47.05,40.01,42.00,45.83,47.09,45.06,40.63,47.42,41.18,46.84,54.22,42.36,46.22,50.23,35.80,42.57,48.69,39.91,42.10,45.86,44.24,45.71,47.24,38.20,50.10,51.25,42.04,42.06,49.14,52.05,43.16,39.36,48.82,36.70,43.91,41.64,44.47,41.62,39.86,44.97,42.80,40.89,48.10,45.01,39.57,46.40,48.06,41.13,46.24,48.44,41.68,46.99,37.90,48.25,41.10,40.00,45.99,41.68,35.24,45.10,45.01,41.41,44.78,54.23,39.96,44.63,46.89,42.28,39.28,47.97,50.16,39.30,46.64,42.43,41.74,43.72,48.18,44.70,47.27,49.91,39.53,39.78,41.78,39.44,52.38,38.70,58.08,44.72,48.14,41.60,40.15,42.25,41.77,44.43,38.83,41.26,43.40,46.88,39.68,43.19,48.66,44.88,39.35,47.73,44.19,48.53,48.07,39.60,45.24,52.45,47.16,43.97,43.28,39.72,41.63,41.07,39.66,46.48,46.15,49.01,46.53,42.63,46.35,46.90,43.61,40.57,44.73,37.51,43.42,44.81,43.11,44.14,44.07,38.61,42.00,46.09,45.13,40.39,46.06,46.23,40.78,53.31,46.00,48.56,40.94,45.06,49.78,44.20,51.09,45.41,46.17,40.88,38.70,39.24,42.61,45.02,42.44,46.66,37.61,44.90,45.85,41.64,45.15,39.23,44.06,41.88,47.58,40.02,48.25,45.13,47.39,44.99,49.35,43.22,47.58,39.41,52.79,41.31,37.34,45.22,43.02,39.99,43.90,45.33,44.82,50.50,41.75,34.94,48.60,43.03,40.74,44.15,43.28,47.75,37.88,45.63,47.66,39.62,39.98,43.56,40.97,42.24,47.26,44.72,40.18,45.30,42.90,43.03,45.85,43.96,41.07,49.44,44.00,42.09,44.47,41.51,45.27,51.35,50.45,42.92,49.81,41.80,45.95,43.56,37.91,44.10,47.44,36.49,39.35,42.83,34.48,41.14,49.07,41.00,50.96,42.56,39.04,43.23,44.44,45.17,43.40,45.97,46.32,46.51,55.72,44.72,46.29,45.53,46.45,40.53,48.33,53.51,41.66,38.36,45.24,46.23,42.48,39.56,44.71,35.58,44.48,47.97,43.89,51.39,38.57,44.18,42.46,47.51,56.26,46.18,38.61,45.14,54.51,45.18,46.50,44.98,49.45,53.64,42.16,46.45,40.91,43.80,50.64,52.49,45.85,42.91,50.60,45.86,40.63,53.30,41.71,39.41,44.09,37.02,48.30,42.11,47.04,45.52,46.32,35.79,45.81,43.01,44.84,41.89,45.53,37.13,45.88,42.76,43.06,44.22,39.47,43.72,48.01,40.93,41.27,45.04,42.11,42.12,37.98,44.99,43.34,43.40,44.21,42.32,43.31,50.63,42.99,38.69,43.34,41.29,41.68,47.10,43.92,41.79,43.92,39.24,45.03,49.04,45.01,49.85,37.64,41.18,42.11,44.83,44.64,39.24,40.95,43.92,36.64,45.65,44.80,43.65,41.16,44.31,39.77,40.40,42.29,38.30,50.59,41.69,49.63,42.38,41.20,39.46,40.72,41.16,55.84,44.67,40.52,45.53,38.87,49.71,42.23,48.62,38.60,47.75,40.48,43.34,41.87,47.63,44.25,42.97,42.14,43.61,41.06,39.92,45.57,44.64,52.18,41.92,42.32,45.69,44.39,39.42,40.74,42.52,47.40,44.81,49.76,42.01,46.66,39.62,42.97,52.38,41.44,49.48,40.38,43.99,48.96,44.24,52.32,43.69,46.24,47.98,51.00,41.41,42.31,47.21,44.37,38.73,51.04,43.93,56.00,44.00,49.84,53.44,47.47,39.23,45.72,46.97,41.27,42.07,42.46,43.74,47.97,42.31,50.23,44.02,44.92,45.07,41.59,42.33,41.87,44.15,49.23,39.65,42.45,52.60,37.91,44.39,51.88,45.51,40.77,43.82,52.06,44.71,53.77,42.56,42.42,53.19,44.98,41.35,38.57,46.70,40.13,41.09,51.07,36.84,45.76,37.27,51.06,40.19,36.22,43.44,38.94,47.29,50.50,44.65,39.76,42.29,42.44,55.51,40.42,44.79,36.77,38.56,42.48,50.89,45.95,36.81,42.81,46.58,44.45,49.14,53.36,46.16,46.77,46.59,43.94,41.57,52.53,46.86,42.27,40.99,44.48,42.36,44.22,51.98,42.32,39.13,44.10,42.97,38.24,41.53,41.54,43.34,45.37,42.94,40.58,44.50,42.18,38.49,47.27,49.17,41.80,46.01,45.89,46.26,46.33,45.53,46.41,47.70,38.17,48.36,38.16,46.21,46.97,38.99,48.57,41.88,46.09,53.24,45.15,38.86,45.79,42.26,41.89,45.15,40.47,51.09,42.38,44.11,48.44,45.55,44.68,51.09,45.97,41.37,43.94,42.76,47.74,48.18,46.20,58.84,39.69,48.51,43.02,47.09,45.38,47.31,44.45,52.34,41.41,39.78,49.01,49.96,35.97,39.29,46.45,53.17,47.53,43.51,41.80,43.86,44.66,42.83,48.05,41.95,48.89,50.10,44.77,42.51,46.02,47.88,40.93,44.95,40.40,43.67,36.42,39.95,52.13,45.10,42.92,45.34,46.02,51.28,46.25,52.32,39.89,39.83,41.87,43.71,39.05,42.64,44.98,42.94,43.10,43.46,45.94,43.77,48.59,47.46,44.29,45.16,46.46,35.41,44.64,50.46,41.96,38.53,42.16,43.94,45.19,42.16,58.33,39.18,42.77,42.08,43.14,51.26,47.95,47.75,49.20,40.46,45.56,42.60,53.83,48.93,44.87,51.89,40.50,46.11,48.18,40.66,40.40,43.94,41.68,44.48,45.76,55.29,54.18,46.79,41.37,43.49,43.47,48.80,43.66,45.33,44.00,49.82,42.80,38.71,46.87,49.77,43.24,46.38,39.02,41.10,44.25,41.12,45.20,41.25,46.96,48.60,44.78,44.05,41.02,50.48,42.29,47.34,42.42,51.98,39.26,41.17,44.18,46.02,43.38,38.86,42.20,44.81,45.64,43.88,42.39,47.90,38.23,45.47,41.81,43.75,50.94,46.31,57.19,48.02,40.22,46.10,45.72,44.50,38.76,41.00,39.60,55.89,45.11,47.45,43.11,42.25,57.22,36.56,47.84,41.06,39.01,57.83,44.69,39.17,39.41,47.97,47.38,38.99,35.85,42.75,48.23,55.11,45.15,41.02,52.95,47.91,49.96,39.13,37.23,41.97,38.10,42.19,42.46,53.63,47.88,56.71,46.03,39.07,43.55,43.55,42.58,40.14,42.41,44.59,43.63,54.68,39.29,41.59,44.62,43.41,44.93,43.66,45.29,44.88,41.86,46.17,49.22,39.65,42.93,41.97,43.16,56.34,45.18,39.18,41.85,39.98,44.18,47.11,36.22,41.58,44.10,41.80,44.18,42.51,50.64,44.93,42.79,45.00,51.14,48.71,42.44,42.96,41.96,50.67,45.99,44.72,42.81,49.54,43.67,45.95,40.65,42.34,52.58,48.49,57.01,45.06,45.55,45.24,41.57,45.53,38.73,49.53,42.13,46.60,40.93,45.21,39.77,43.61,48.50,40.28,43.95,43.91,43.78,54.66,41.57,36.12,45.37,40.24,46.95,40.97,50.99,60.15,43.68,44.51,48.26,37.99,36.50,48.19,41.85,40.73,47.07,56.92,38.76,43.06,44.94,41.31,42.70,43.60,43.01,42.66,45.45,51.39,42.90,44.32,44.14,42.30,45.30,42.58,43.90,53.94,42.47,57.70,40.63,41.61,44.57,44.87,49.98,56.33,43.08,41.10,48.41,45.97,40.25,51.13,51.03,43.51,42.37,40.38,43.45,44.42,44.62,43.98,36.25,44.89,42.12,45.56,42.81,41.54,46.30,43.86,43.09,43.69,43.94,46.76,47.61,42.50,41.95,52.20,46.71,42.09,45.70,46.92,43.28,47.55,49.70,39.21,44.56,40.90,47.18,49.53,44.38,44.91,50.57,49.21,43.13,46.35,42.91,46.58,41.21,48.41,45.23,41.08,45.06,47.74,47.12,37.93,37.07,54.12,39.78,42.78,44.61,43.99,40.37,49.42,51.04,38.88,45.32,46.38,41.05,39.39,45.52,46.53,42.00,43.50,42.57,50.11,48.91,44.69,39.56,45.58,44.75,47.03,41.36,40.31,47.94,40.92,43.37,44.95,43.33,43.96,47.44,38.01,39.48,44.85,45.35,44.22,46.04,47.19,44.69,41.86,48.86,48.33,41.75,52.70,45.17,46.75,39.94,46.81,44.31,37.78,43.78,44.30,44.06,42.15,43.67,41.22,42.67,48.01,41.58,41.09,43.17,39.70,40.29,47.40,45.06,42.27,39.48,42.58,47.06,41.51,48.47,48.48,45.93,45.52,45.37,49.06,39.12,40.35,42.72,46.14,41.88,46.39,49.08,43.85,46.94,43.50,40.97,42.76,40.84,46.08,45.01,43.04,47.08,40.90,42.82,44.51,45.48,53.42,48.66,43.22,48.76,37.95,50.73,55.74,44.21,40.84,42.71,39.19,50.37,46.93,39.96,44.32,43.72,40.74,44.94,42.98,45.27,40.01,38.39,49.85,46.91,50.00,42.63,39.67,43.64,46.71,44.20,41.67,44.51,42.31,40.19,49.29,41.87,39.34,42.01,41.01,42.45,51.33,39.34,44.64,39.44,52.31,45.45,48.40,41.56,48.71,48.22,39.71,39.24,44.29,48.80,44.90,49.95,52.32,43.50,47.63,44.93,45.58,46.31,45.02,40.56,45.23,45.57,38.40,42.55,42.40,52.17,48.95,50.29,40.60,39.03,40.27,39.54,38.17,48.50,41.74,42.42,42.44,46.34,42.08,46.08,41.37,41.04,43.32,43.48,40.95,39.68,40.84,47.46,54.90,40.16,53.30,47.42,43.65,50.90,43.46,41.08,46.87,35.19,48.65,45.44,44.43,40.16,41.45,48.66
2022-09-01 20:37:18 [/home/zhao/code/spex+/libs/trainer.py:245 - INFO ] START FROM EPOCH 0, LOSS = 44.4785
2022-09-01 20:37:18 [/home/zhao/code/spex+/libs/trainer.py:185 - INFO ] Set train mode...
